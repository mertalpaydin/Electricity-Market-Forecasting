{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronos2 Model Training\n",
    "\n",
    "Train and evaluate Chronos2 foundation model for electricity price forecasting.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Subsampling strategy\n",
    "- Baseline models (to establish performance target)\n",
    "- Best hyperparameters from Optuna\n",
    "\n",
    "**Goal**: Beat best baseline model within 3 hours (primarily zero-shot evaluation)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from chronos import Chronos2Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    project_root = os.path.abspath('..')\n",
    "else:\n",
    "    project_root = os.getcwd()\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.datamodule import ElectricityDataModule, masked_smoothed_smape\n",
    "from src.helper import _to_tensor, _to_torch, _extract_context_target_mask, align_forecast_to_target\n",
    "\n",
    "\n",
    "# Set precision for Tensor Cores to improve performance\n",
    "torch.backends.fp32_precision = \"medium\"\n",
    "torch.backends.cuda.matmul.fp32_precision = \"medium\"\n",
    "torch.backends.cudnn.fp32_precision = \"medium\"\n",
    "torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.rnn.fp32_precision = \"tf32\"\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Load Best Hyperparameters\n",
    "\n",
    "**Note**: This notebook uses the full dataset filtered by `is_trading` feature, not the limited subset used for Optuna tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best baseline sMAPE: 71.41%\n",
      "Chronos2 must beat this!\n",
      "\n",
      "Chronos2 Configuration:\n",
      "{\n",
      "  \"quantile_levels\": \"medium\",\n",
      "  \"batch_size\": 64,\n",
      "  \"lr\": 0.0003,\n",
      "  \"max_context_length\": 96,\n",
      "  \"actual_quantile_levels\": [\n",
      "    0.05,\n",
      "    0.25,\n",
      "    0.5,\n",
      "    0.75,\n",
      "    0.95\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"..\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "TRAIN_DIR_FILTERED = os.path.join(DATA_DIR, \"train_trading_only\")\n",
    "VAL_DIR_FILTERED = os.path.join(DATA_DIR, \"val_trading_only\")\n",
    "SCALERS_DIR = os.path.join(DATA_DIR, \"scalers\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COLS = [\"high\", \"low\", \"close\", \"volume\"]\n",
    "OUTPUT_CHUNK_LENGTH = 10\n",
    "SEED = 827\n",
    "\n",
    "# Load best Chronos2 hyperparameters from tuning\n",
    "with open(os.path.join(RESULTS_DIR, \"best_params_chronos2.json\"), \"r\") as f:\n",
    "    BEST_CHRONOS_PARAMS = json.load(f)\n",
    "\n",
    "# Load baseline results to establish target\n",
    "baseline_file = os.path.join(RESULTS_DIR, \"baseline_summary.json\")\n",
    "if os.path.exists(baseline_file):\n",
    "    with open(baseline_file, \"r\") as f:\n",
    "        baseline_summary = json.load(f)\n",
    "    \n",
    "    # Find the best sMAPE from the existing baseline models\n",
    "    baseline_smapes = [m['overall_smape'] for m in baseline_summary.get('models', {}).values()]\n",
    "    BASELINE_TARGET = min(baseline_smapes) if baseline_smapes else float('inf')\n",
    "    print(f\"Best baseline sMAPE: {BASELINE_TARGET:.2f}%\")\n",
    "    print(f\"Chronos2 must beat this!\\n\")\n",
    "else:\n",
    "    BASELINE_TARGET = float('inf')\n",
    "    print(\"Warning: No baseline results found\\n\")\n",
    "\n",
    "# Extract hyperparameters\n",
    "BATCH_SIZE = BEST_CHRONOS_PARAMS['batch_size']\n",
    "LEARNING_RATE = BEST_CHRONOS_PARAMS['lr']\n",
    "MAX_CONTEXT_LENGTH = BEST_CHRONOS_PARAMS['max_context_length']\n",
    "INPUT_CHUNK_LENGTH = min(48, MAX_CONTEXT_LENGTH)\n",
    "QUANTILE_LEVELS = BEST_CHRONOS_PARAMS.get('actual_quantile_levels', [0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "\n",
    "print(\"Chronos2 Configuration:\")\n",
    "print(json.dumps(BEST_CHRONOS_PARAMS, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data (Full Dataset with Trading-Only Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded subsampling recommendation: Stratified\n",
      "  Val Loss: 8.9979\n",
      "  Epochs: 13\n",
      "\n",
      "Found only 3/672 assets - refreshing with full dataset...\n",
      "================================================================================\n",
      "CREATING SUBSAMPLED FILES FOR ALL ASSETS\n",
      "================================================================================\n",
      "Strategy: Stratified\n",
      "Strategy params: {'non_trading_fraction': 0.1}\n",
      "Found 672 assets\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675a254792604b189636923da86cf0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing assets:   0%|          | 0/672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "Assets processed: 672\n",
      "Total training rows: 9,485,760\n",
      "Total validation rows: 4,666,410\n",
      "================================================================================\n",
      "\n",
      "✓ Created subsampled files for 672 assets using Stratified strategy\n",
      "\n",
      "Found 672 assets in filtered data.\n",
      "DataModule configured and set up with Stratified strategy.\n"
     ]
    }
   ],
   "source": [
    "def strategy_trading_only(df):\n",
    "    \"\"\"Keep only trading periods.\"\"\"\n",
    "    return df[df['is_trading'] == 1].copy()\n",
    "\n",
    "\n",
    "def strategy_stratified(df, non_trading_fraction=0.1):\n",
    "    \"\"\"Keep all trading + fraction of non-trading.\"\"\"\n",
    "    trading_df = df[df['is_trading'] == 1].copy()\n",
    "    non_trading_df = df[df['is_trading'] == 0].copy()\n",
    "    \n",
    "    # Sample non-trading rows\n",
    "    n_sample = int(len(non_trading_df) * non_trading_fraction)\n",
    "    if n_sample > 0:\n",
    "        non_trading_sample = non_trading_df.sample(n=n_sample, random_state=SEED)\n",
    "    else:\n",
    "        non_trading_sample = pd.DataFrame()\n",
    "    \n",
    "    # Combine and sort by time\n",
    "    if len(non_trading_sample) > 0:\n",
    "        combined = pd.concat([trading_df, non_trading_sample])\n",
    "        combined = combined.sort_values('ExecutionTime').reset_index(drop=True)\n",
    "        return combined\n",
    "    else:\n",
    "        return trading_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def strategy_boundary_aware(df, window_before=4, window_after=4):\n",
    "    \"\"\"Keep trading + periods around trading boundaries.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['keep'] = False\n",
    "    \n",
    "    # Keep all trading rows\n",
    "    df.loc[df['is_trading'] == 1, 'keep'] = True\n",
    "    \n",
    "    # Find boundaries\n",
    "    trading = df['is_trading'].values\n",
    "    changes = np.diff(trading, prepend=0)\n",
    "    \n",
    "    starts = np.where(changes == 1)[0]\n",
    "    stops = np.where(changes == -1)[0]\n",
    "    \n",
    "    # Mark rows around boundaries\n",
    "    for start in starts:\n",
    "        begin = max(0, start - window_before)\n",
    "        df.loc[begin:start, 'keep'] = True\n",
    "    \n",
    "    for stop in stops:\n",
    "        end = min(len(df), stop + window_after)\n",
    "        df.loc[stop:end, 'keep'] = True\n",
    "    \n",
    "    return df[df['keep']].drop(columns=['keep']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def strategy_hybrid(df, non_trading_fraction=0.05, window_before=4, window_after=4):\n",
    "    \"\"\"Combination of boundary-aware + stratified sampling.\"\"\"\n",
    "    df_boundary = strategy_boundary_aware(df, window_before, window_after)\n",
    "    \n",
    "    kept_indices = set(df_boundary.index)\n",
    "    non_trading_df = df[(df['is_trading'] == 0) & (~df.index.isin(kept_indices))].copy()\n",
    "    \n",
    "    if len(non_trading_df) > 0:\n",
    "        n_sample = int(len(non_trading_df) * non_trading_fraction)\n",
    "        if n_sample > 0:\n",
    "            non_trading_sample = non_trading_df.sample(n=n_sample, random_state=SEED)\n",
    "            combined = pd.concat([df_boundary, non_trading_sample])\n",
    "            combined = combined.sort_values('ExecutionTime').reset_index(drop=True)\n",
    "            return combined\n",
    "    \n",
    "    return df_boundary\n",
    "\n",
    "\n",
    "def apply_subsampling_strategy(df, strategy_name, **kwargs):\n",
    "    \"\"\"Apply the specified subsampling strategy to a dataframe.\"\"\"\n",
    "    if strategy_name == 'Trading-Only':\n",
    "        return strategy_trading_only(df)\n",
    "    elif strategy_name == 'Stratified':\n",
    "        return strategy_stratified(df, kwargs.get('non_trading_fraction', 0.1))\n",
    "    elif strategy_name == 'Boundary-Aware':\n",
    "        return strategy_boundary_aware(df, kwargs.get('window_before', 4), kwargs.get('window_after', 4))\n",
    "    elif strategy_name == 'Hybrid':\n",
    "        return strategy_hybrid(df, kwargs.get('non_trading_fraction', 0.05), \n",
    "                              kwargs.get('window_before', 4), kwargs.get('window_after', 4))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy_name}\")\n",
    "\n",
    "\n",
    "def create_subsampled_files_for_all_assets(train_dir, val_dir, output_train_dir, output_val_dir, \n",
    "                                           strategy_name, **strategy_kwargs):\n",
    "    \"\"\"\n",
    "    Apply subsampling strategy to ALL assets and create filtered files.\n",
    "    \n",
    "    This is for full model training, not Optuna tuning.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_train_dir, exist_ok=True)\n",
    "    os.makedirs(output_val_dir, exist_ok=True)\n",
    "    \n",
    "    train_files = [f for f in os.listdir(train_dir) if f.endswith('.parquet')]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CREATING SUBSAMPLED FILES FOR ALL ASSETS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Strategy: {strategy_name}\")\n",
    "    print(f\"Strategy params: {strategy_kwargs}\")\n",
    "    print(f\"Found {len(train_files)} assets\")\n",
    "    print()\n",
    "    \n",
    "    processed_count = 0\n",
    "    total_train_rows = 0\n",
    "    total_val_rows = 0\n",
    "    skipped_assets = []\n",
    "    \n",
    "    for asset_file in tqdm(train_files, desc=\"Processing assets\"):\n",
    "        asset_name = asset_file.replace('.parquet', '')\n",
    "        \n",
    "        try:\n",
    "            # Process training file\n",
    "            train_path = os.path.join(train_dir, asset_file)\n",
    "            if os.path.exists(train_path):\n",
    "                df = pd.read_parquet(train_path)\n",
    "                df_sampled = apply_subsampling_strategy(df, strategy_name, **strategy_kwargs)\n",
    "                \n",
    "                if len(df_sampled) > 0:\n",
    "                    output_path = os.path.join(output_train_dir, asset_file)\n",
    "                    df_sampled.to_parquet(output_path)\n",
    "                    total_train_rows += len(df_sampled)\n",
    "                    processed_count += 1\n",
    "                else:\n",
    "                    skipped_assets.append(asset_name)\n",
    "            \n",
    "            # Process validation file\n",
    "            val_path = os.path.join(val_dir, asset_file)\n",
    "            if os.path.exists(val_path):\n",
    "                df = pd.read_parquet(val_path)\n",
    "                df_sampled = apply_subsampling_strategy(df, strategy_name, **strategy_kwargs)\n",
    "                \n",
    "                if len(df_sampled) > 0:\n",
    "                    output_path = os.path.join(output_val_dir, asset_file)\n",
    "                    df_sampled.to_parquet(output_path)\n",
    "                    total_val_rows += len(df_sampled)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {asset_name}: {e}\")\n",
    "            skipped_assets.append(asset_name)\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Assets processed: {processed_count}\")\n",
    "    print(f\"Total training rows: {total_train_rows:,}\")\n",
    "    print(f\"Total validation rows: {total_val_rows:,}\")\n",
    "    if skipped_assets:\n",
    "        print(f\"Skipped assets ({len(skipped_assets)}): {', '.join(skipped_assets[:10])}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "\n",
    "# Load subsampling recommendation\n",
    "recommendation_file = os.path.join(RESULTS_DIR, \"subsampling_recommendation.json\")\n",
    "if os.path.exists(recommendation_file):\n",
    "    with open(recommendation_file, \"r\") as f:\n",
    "        recommendation = json.load(f)\n",
    "    SUBSAMPLING_STRATEGY = recommendation['best_strategy']\n",
    "    print(f\"✓ Loaded subsampling recommendation: {SUBSAMPLING_STRATEGY}\")\n",
    "    print(f\"  Val Loss: {recommendation['val_loss']:.4f}\")\n",
    "    print(f\"  Epochs: {recommendation['epochs_trained']}\\n\")\n",
    "else:\n",
    "    # Fallback to Stratified if no recommendation\n",
    "    SUBSAMPLING_STRATEGY = \"Stratified\"\n",
    "    print(f\"⚠ No subsampling recommendation found, using default: {SUBSAMPLING_STRATEGY}\\n\")\n",
    "\n",
    "# Define full data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "\n",
    "# Check if we need to create/refresh the filtered files\n",
    "needs_refresh = False\n",
    "if not os.path.exists(TRAIN_DIR_FILTERED) or not os.listdir(TRAIN_DIR_FILTERED):\n",
    "    needs_refresh = True\n",
    "    print(\"No filtered data found - creating new filtered dataset...\")\n",
    "else:\n",
    "    # Check if we have limited files (from Optuna) or full dataset\n",
    "    existing_train = [f for f in os.listdir(TRAIN_DIR_FILTERED) if f.endswith('.parquet')]\n",
    "    all_train = [f for f in os.listdir(TRAIN_DIR) if f.endswith('.parquet')]\n",
    "    \n",
    "    if len(existing_train) < len(all_train) * 0.9:  # Less than 90% of full dataset\n",
    "        print(f\"Found only {len(existing_train)}/{len(all_train)} assets - refreshing with full dataset...\")\n",
    "        needs_refresh = True\n",
    "    else:\n",
    "        print(f\"✓ Using existing {len(existing_train)} filtered assets\")\n",
    "\n",
    "if needs_refresh:\n",
    "    # Set strategy-specific parameters\n",
    "    strategy_params = {}\n",
    "    if SUBSAMPLING_STRATEGY == \"Stratified\":\n",
    "        strategy_params['non_trading_fraction'] = 0.1\n",
    "    elif SUBSAMPLING_STRATEGY == \"Hybrid\":\n",
    "        strategy_params['non_trading_fraction'] = 0.05\n",
    "        strategy_params['window_before'] = 4\n",
    "        strategy_params['window_after'] = 4\n",
    "    elif SUBSAMPLING_STRATEGY == \"Boundary-Aware\":\n",
    "        strategy_params['window_before'] = 4\n",
    "        strategy_params['window_after'] = 4\n",
    "    \n",
    "    processed = create_subsampled_files_for_all_assets(\n",
    "        TRAIN_DIR, \n",
    "        VAL_DIR, \n",
    "        TRAIN_DIR_FILTERED, \n",
    "        VAL_DIR_FILTERED,\n",
    "        SUBSAMPLING_STRATEGY,\n",
    "        **strategy_params\n",
    "    )\n",
    "    print(f\"✓ Created subsampled files for {processed} assets using {SUBSAMPLING_STRATEGY} strategy\")\n",
    "\n",
    "# Load assets list from filtered data\n",
    "if not os.path.exists(TRAIN_DIR_FILTERED) or not os.listdir(TRAIN_DIR_FILTERED):\n",
    "    raise FileNotFoundError(f\"Missing filtered data at {TRAIN_DIR_FILTERED}\")\n",
    "\n",
    "assets_list = [f.replace('.parquet', '') for f in os.listdir(TRAIN_DIR_FILTERED) if f.endswith('.parquet')]\n",
    "print(f\"\\nFound {len(assets_list)} assets in filtered data.\")\n",
    "\n",
    "datamodule = ElectricityDataModule(\n",
    "    train_parquet=TRAIN_DIR_FILTERED,\n",
    "    val_parquet=VAL_DIR_FILTERED,\n",
    "    scalers_dir=SCALERS_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    dataset_kwargs={\n",
    "        'input_chunk_length': INPUT_CHUNK_LENGTH,\n",
    "        'output_chunk_length': OUTPUT_CHUNK_LENGTH,\n",
    "        'target_cols': TARGET_COLS,\n",
    "        'assets_list': assets_list\n",
    "    }\n",
    ")\n",
    "datamodule.setup()\n",
    "print(f\"DataModule configured and set up with {SUBSAMPLING_STRATEGY} strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Load Chronos2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chronos2 pipeline...\n",
      "Pipeline loaded on device: cuda\n",
      "\n",
      "================================================================================\n",
      "ZERO-SHOT CHRONOS2 EVALUATION\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b32c365c624bf3ab42f5e0b81e5fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot sMAPE (100 batches): 72.53%\n",
      "Baseline Target: 71.41%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Chronos2 pipeline...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = Chronos2Pipeline.from_pretrained(\n",
    "    \"amazon/chronos-2\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else None,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "print(f\"Pipeline loaded on device: {device}\")\n",
    "\n",
    "def evaluate_chronos(model_pipeline, dataloader, quantile_levels, max_batches=None):\n",
    "    \"\"\"Evaluates a Chronos pipeline, returning avg sMAPE and per-batch losses.\"\"\"\n",
    "    model_pipeline.model.eval()\n",
    "    batch_losses = []\n",
    "    \n",
    "    val_loader_tqdm = tqdm(dataloader, total=max_batches or len(dataloader), desc=\"Evaluating\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader_tqdm):\n",
    "            if max_batches and i >= max_batches:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Extract batch components\n",
    "                raw_context, raw_target, raw_mask = _extract_context_target_mask(batch)\n",
    "                \n",
    "                # Convert to tensors (CPU)\n",
    "                context = _to_tensor(raw_context)\n",
    "                target = _to_tensor(raw_target) if raw_target is not None else None\n",
    "                future_mask = _to_tensor(raw_mask) if raw_mask is not None else None\n",
    "                \n",
    "                # Ensure 3D: [B, seq_len, n_features]\n",
    "                if context.ndim == 2:\n",
    "                    context = context.unsqueeze(-1)\n",
    "                if target is not None and target.ndim == 2:\n",
    "                    target = target.unsqueeze(-1)\n",
    "                \n",
    "                # Get shapes\n",
    "                B = context.shape[0]\n",
    "                context_length = context.shape[1]\n",
    "                n_features = context.shape[2]\n",
    "                \n",
    "                # Prepare input for pipeline: [B, n_features, history_len]\n",
    "                multivariate_context = context.transpose(1, 2)\n",
    "                \n",
    "                # Call pipeline with numpy input\n",
    "                q_out_raw, mean_out_raw = model_pipeline.predict_quantiles(\n",
    "                    multivariate_context.cpu().numpy(),\n",
    "                    prediction_length=OUTPUT_CHUNK_LENGTH,\n",
    "                    quantile_levels=quantile_levels\n",
    "                )\n",
    "                \n",
    "                # Convert to torch tensors\n",
    "                q_out = _to_torch(q_out_raw)\n",
    "                mean_out = _to_torch(mean_out_raw) if mean_out_raw is not None else None\n",
    "                \n",
    "                # Align forecast to [B, pred_len, n_features]\n",
    "                forecast_median = align_forecast_to_target(\n",
    "                    q_out, mean_out, n_features, OUTPUT_CHUNK_LENGTH, quantile_levels\n",
    "                )\n",
    "                \n",
    "                if forecast_median is None or target is None:\n",
    "                    continue\n",
    "                \n",
    "                if forecast_median.shape != target.shape:\n",
    "                    continue\n",
    "                \n",
    "                # Move to device for loss computation\n",
    "                forecast_median = forecast_median.to(device)\n",
    "                target = target.to(device)\n",
    "                future_mask = (future_mask.to(device) if future_mask is not None \n",
    "                              else torch.ones_like(target).to(device))\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = masked_smoothed_smape(forecast_median, target, future_mask)\n",
    "                batch_losses.append(loss.item())\n",
    "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    avg_smape = np.mean(batch_losses) * 100 if batch_losses else float('inf')\n",
    "    return avg_smape, batch_losses\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT CHRONOS2 EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "val_loader = datamodule.val_dataloader()\n",
    "zero_shot_smape, _ = evaluate_chronos(pipeline, val_loader, QUANTILE_LEVELS, max_batches=100)\n",
    "print(f\"\\nZero-Shot sMAPE (100 batches): {zero_shot_smape:.2f}%\")\n",
    "print(f\"Baseline Target: {BASELINE_TARGET:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Fine-Tuning with Gradual Unfreezing\n\n**Architecture-Aware Fine-Tuning Strategy:**\n\nChronos2 has the following structure:\n- `input_patch_embedding`: Input embedding layer\n- `encoder.block`: 12 transformer encoder blocks  \n- `output_patch_embedding`: Output prediction head\n\n**3-Stage Gradual Unfreezing:**\n1. **Stage 1 (5 epochs)**: Train only `output_patch_embedding` \n2. **Stage 2 (8 epochs)**: Unfreeze last 3 encoder blocks with differential LR\n3. **Stage 3 (12 epochs)**: Full fine-tuning with layerwise LR decay\n\nThis approach prevents catastrophic forgetting of the pre-trained knowledge."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_training_stage(model, optimizer, train_loader, val_loader, epochs, stage_name, history_list):\n    \"\"\"Runs one stage of training and validation.\"\"\"\n    print(f\"\\n{'='*80}\\nStarting {stage_name}\\n{'='*80}\")\n    for epoch in range(epochs):\n        # --- Training Loop ---\n        model.train()\n        train_losses = []\n        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        for batch in train_loader_tqdm:\n            try:\n                context, target, mask = _extract_context_target_mask(batch)\n                context = _to_tensor(context).to(device)\n                target = _to_tensor(target).to(device)\n                mask = _to_tensor(mask).to(device) if mask is not None else torch.ones_like(target).to(device)\n                \n                # Ensure 3D: [B, seq_len, n_features]\n                if context.ndim == 2:\n                    context = context.unsqueeze(-1)\n                if target.ndim == 2:\n                    target = target.unsqueeze(-1)\n                \n                B, seq_len, n_features = context.shape\n                \n                # Prepare input for Chronos2: [B, n_features, seq_len]\n                multivariate_context = context.transpose(1, 2)\n                \n                optimizer.zero_grad()\n                \n                # Forward pass - model expects [B, n_features, seq_len] tensor\n                output = model(\n                    context=multivariate_context,\n                    num_output_patches=OUTPUT_CHUNK_LENGTH\n                )\n                \n                # Extract prediction from Chronos2Output\n                # output.prediction has shape [B, n_features, pred_len, n_quantiles]\n                pred = output.prediction\n                \n                # Get median quantile (index 2 for [0.05, 0.25, 0.5, 0.75, 0.95])\n                median_idx = len(QUANTILE_LEVELS) // 2\n                forecast = pred[:, :, :, median_idx]  # [B, n_features, pred_len]\n                \n                # Transpose to [B, pred_len, n_features] to match target\n                forecast = forecast.transpose(1, 2).contiguous()\n                \n                # Compute loss\n                loss = masked_smoothed_smape(forecast, target, mask)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                train_losses.append(loss.item())\n                train_loader_tqdm.set_postfix(train_loss=np.mean(train_losses))\n            except Exception as e:\n                print(f\"Error in training batch: {e}\")\n                import traceback\n                traceback.print_exc()\n                continue\n\n        # --- Validation Loop ---\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n                try:\n                    context, target, mask = _extract_context_target_mask(batch)\n                    context = _to_tensor(context).to(device)\n                    target = _to_tensor(target).to(device)\n                    mask = _to_tensor(mask).to(device) if mask is not None else torch.ones_like(target).to(device)\n                    \n                    # Ensure 3D\n                    if context.ndim == 2:\n                        context = context.unsqueeze(-1)\n                    if target.ndim == 2:\n                        target = target.unsqueeze(-1)\n                    \n                    # Prepare input\n                    multivariate_context = context.transpose(1, 2)\n                    \n                    # Forward pass\n                    output = model(\n                        context=multivariate_context,\n                        num_output_patches=OUTPUT_CHUNK_LENGTH\n                    )\n                    \n                    # Extract median prediction\n                    pred = output.prediction\n                    median_idx = len(QUANTILE_LEVELS) // 2\n                    forecast = pred[:, :, :, median_idx]\n                    forecast = forecast.transpose(1, 2).contiguous()\n                    \n                    loss = masked_smoothed_smape(forecast, target, mask)\n                    val_losses.append(loss.item())\n                except Exception as e:\n                    print(f\"Error in validation batch: {e}\")\n                    continue\n        \n        avg_train_loss = np.mean(train_losses) if train_losses else float('inf')\n        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')\n        history_list.append({'stage': stage_name, 'epoch': epoch, 'train_loss': avg_train_loss, 'val_loss': avg_val_loss})\n        print(f\"Epoch {epoch+1} Summary: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\ndef finetune_chronos_gradual(model_pipeline, train_dl, val_dl, base_lr):\n    \"\"\"Implements the 3-stage gradual unfreezing and fine-tuning process.\"\"\"\n    model = model_pipeline.model\n    all_history = []\n\n    # --- Stage 1: Output Embedding Only ---\n    print(f\"\\n{'='*80}\")\n    print(\"STAGE 1: Training output_patch_embedding only\")\n    print(f\"{'='*80}\")\n    \n    for param in model.parameters():\n        param.requires_grad = False\n    for param in model.output_patch_embedding.parameters():\n        param.requires_grad = True\n    \n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n    \n    optimizer = torch.optim.AdamW(trainable_params, lr=base_lr)\n    run_training_stage(model, optimizer, train_dl, val_dl, epochs=5, stage_name=\"1-OutputHead\", history_list=all_history)\n\n    # --- Stage 2: Partial Unfreeze (last 3 encoder blocks) ---\n    print(f\"\\n{'='*80}\")\n    print(\"STAGE 2: Unfreezing last 3 encoder blocks\")\n    print(f\"{'='*80}\")\n    \n    # Unfreeze last 3 encoder blocks\n    for block in model.encoder.block[-3:]:\n        for param in block.parameters():\n            param.requires_grad = True\n    \n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n            \n    optimizer = torch.optim.AdamW([\n        {'params': model.output_patch_embedding.parameters(), 'lr': base_lr},\n        {'params': [p for block in model.encoder.block[-3:] for p in block.parameters()], 'lr': base_lr / 10},\n    ])\n    run_training_stage(model, optimizer, train_dl, val_dl, epochs=8, stage_name=\"2-PartialUnfreeze\", history_list=all_history)\n\n    # --- Stage 3: Full Fine-tune ---\n    print(f\"\\n{'='*80}\")\n    print(\"STAGE 3: Full fine-tuning with differential learning rates\")\n    print(f\"{'='*80}\")\n    \n    for param in model.parameters():\n        param.requires_grad = True\n    \n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n        \n    optimizer = torch.optim.AdamW([\n        {'params': model.output_patch_embedding.parameters(), 'lr': base_lr / 5},\n        {'params': model.encoder.parameters(), 'lr': base_lr / 50},\n        {'params': model.input_patch_embedding.parameters(), 'lr': base_lr / 100},\n    ])\n    run_training_stage(model, optimizer, train_dl, val_dl, epochs=12, stage_name=\"3-FullFinetune\", history_list=all_history)\n    \n    return all_history\n\n# Run the fine-tuning process\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING CHRONOS2 FINE-TUNING\")\nprint(\"=\"*80)\nprint(f\"Base learning rate: {LEARNING_RATE}\")\nprint(f\"Total epochs: 5 + 8 + 12 = 25\")\nprint(f\"Strategy: Gradual unfreezing with differential learning rates\")\nprint(\"=\"*80)\n\ntrain_loader = datamodule.train_dataloader()\nfinetuning_history = finetune_chronos_gradual(pipeline, train_loader, val_loader, base_lr=LEARNING_RATE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Fine-Tuning Loss Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(finetuning_history)\n",
    "history_df['global_epoch'] = history_df.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plotting loss curves\n",
    "ax.plot(history_df['global_epoch'], history_df['train_loss'], label='Train Loss', color='blue', marker='o', markersize=4)\n",
    "ax.plot(history_df['global_epoch'], history_df['val_loss'], label='Validation Loss', color='orange', marker='x', markersize=5)\n",
    "\n",
    "# Adding vertical lines for each stage\n",
    "stage_boundaries = history_df.drop_duplicates(subset='stage', keep='first')\n",
    "for idx, stage in stage_boundaries.iterrows():\n",
    "    ax.axvline(x=stage['global_epoch'], color='red', linestyle='--', alpha=0.7)\n",
    "    ax.text(stage['global_epoch'] + 0.1, history_df['val_loss'].max() * 0.95, stage['stage'], rotation=90, verticalalignment='top', color='red')\n",
    "\n",
    "ax.set_xlabel('Global Epoch', fontweight='bold')\n",
    "ax.set_ylabel('sMAPE Loss', fontweight='bold')\n",
    "ax.set_title('Chronos2 Fine-Tuning: Loss Evolution', fontweight='bold', fontsize=16)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'chronos2_finetuning_loss.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL FINE-TUNED CHRONOS2 EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "finetuned_smape, _ = evaluate_chronos(pipeline, val_loader, QUANTILE_LEVELS)  # Full evaluation\n",
    "print(f\"\\nFinal Fine-Tuned sMAPE: {finetuned_smape:.2f}%\")\n",
    "\n",
    "# --- Comparison Plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "models = ['Baseline', 'Chronos2\\n(Zero-Shot)', 'Chronos2\\n(Fine-Tuned)']\n",
    "smapes = [BASELINE_TARGET, zero_shot_smape, finetuned_smape]\n",
    "colors = ['#3498db', '#e67e22', '#2ecc71' if finetuned_smape < BASELINE_TARGET else '#e74c3c']\n",
    "\n",
    "bars = ax.bar(models, smapes, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('sMAPE (%)', fontweight='bold')\n",
    "ax.set_title('Chronos2 Fine-Tuning vs. Zero-Shot and Baseline', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'chronos2_final_comparison.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    'model': 'Chronos2-Finetuned',\n",
    "    'hyperparameters': BEST_CHRONOS_PARAMS,\n",
    "    'finetuning_strategy': '3-stage gradual unfreezing',\n",
    "    'zero_shot_smape': float(zero_shot_smape),\n",
    "    'final_finetuned_smape': float(finetuned_smape),\n",
    "    'baseline_target': float(BASELINE_TARGET),\n",
    "    'beats_baseline': bool(finetuned_smape < BASELINE_TARGET),\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'chronos2_finetuned_results.json'), 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "print(f\"Final results saved to {os.path.join(RESULTS_DIR, 'chronos2_finetuned_results.json')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}