{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ec4261-9742-4f03-a22d-a0062b6e48ec",
   "metadata": {},
   "source": [
    "# Systematic Hyperparameter Tuning\n",
    "\n",
    "In this notebook, we use Optuna to find the optimal hyperparameters for our baseline models before running the final training and evaluation. As per the project plan, we will tune the deep learning models on a representative subsample of the data to save time, while tree-based models can be tuned on a larger set if needed.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Asset Subsampling**: We will analyze the liquidity of all assets and create a small, representative subset for tuning.\n",
    "2.  **Optuna Objective Functions**: We will define an objective function for each model architecture (LSTM, Chronos) that Optuna will seek to minimize (the validation sMAPE or loss).\n",
    "3.  **Run Studies**: We will execute the tuning studies for a predefined number of trials.\n",
    "4.  **Save Best Parameters**: The best hyperparameters found will be saved for use in the next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab60fee0-2564-4db3-b052-c6aa5bb71f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The StatsForecast module could not be imported. To enable support for the AutoARIMA, AutoETS and Croston models, please consider installing it.\n",
      "The `XGBoost` module could not be imported. To enable XGBoost support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n",
      "The `XGBoost` module could not be imported. To enable XGBoost support in Darts, follow the detailed instructions in the installation guide: https://github.com/unit8co/darts/blob/master/INSTALL.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, os, time, warnings\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import optuna\n",
    "from darts import TimeSeries\n",
    "from darts.models import BlockRNNModel\n",
    "from chronos import Chronos2Pipeline\n",
    "\n",
    "# Add project root to path to allow imports from src\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    project_root = os.path.abspath('..')\n",
    "else:\n",
    "    project_root = os.getcwd()\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.datamodule import ElectricityDataModule, masked_smoothed_smape\n",
    "\n",
    "# Set precision for Tensor Cores to improve performance\n",
    "torch.backends.fp32_precision = \"medium\"\n",
    "torch.backends.cuda.matmul.fp32_precision = \"medium\"\n",
    "torch.backends.cudnn.fp32_precision = \"medium\"\n",
    "torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.rnn.fp32_precision = \"tf32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010acb99-75e8-4b50-8b94-1c16337627c2",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    " \n",
    "Define paths and constants for the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b71d95-7084-49e6-a773-498da8ce70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project paths\n",
    "BASE_DIR = \"..\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_DIR_FILTERED = os.path.join(DATA_DIR, \"train_trading_only\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "VAL_DIR_FILTERED = os.path.join(DATA_DIR, \"val_trading_only\")\n",
    "SCALERS_DIR = os.path.join(DATA_DIR, \"scalers\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "# Create directories if needed\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_DIR_FILTERED, exist_ok=True)\n",
    "os.makedirs(VAL_DIR_FILTERED, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "INPUT_CHUNK_LENGTH = 48\n",
    "OUTPUT_CHUNK_LENGTH = 10\n",
    "TARGET_COLS = [\"high\", \"low\", \"close\", \"volume\"]\n",
    "N_TRIALS_Chronos = 20  # Number of trials for hyperparameter search\n",
    "N_TRIALS_LSTM = 10\n",
    "SEED = 827\n",
    "\n",
    "# Global variables for caching\n",
    "PREBUILT_DATA = None\n",
    "TUNING_ASSETS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51ba4a-34cc-4a37-b53e-1e320e4da104",
   "metadata": {},
   "source": [
    "## 2. Asset Subsampling for Efficient Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f80d7e8-3e4d-482f-8336-bec69b49c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trading_segments(df, min_segment_length=96):\n",
    "    \"\"\"\n",
    "    Extract continuous trading segments from dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Must have 'is_trading' column and 'ExecutionTime' column\n",
    "    min_segment_length : int\n",
    "        Minimum number of consecutive trading rows to keep (default: 96 = 24 hours for 15min data)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of pd.DataFrame : Each dataframe is a continuous trading segment\n",
    "    \"\"\"\n",
    "    if 'is_trading' not in df.columns:\n",
    "        print(\"Warning: No 'is_trading' column, using volume > 0\")\n",
    "        df['is_trading'] = (df['volume'] > 0).astype(int)\n",
    "    \n",
    "    # Find where trading starts and stops\n",
    "    df = df.sort_values('ExecutionTime').reset_index(drop=True)\n",
    "    trading = df['is_trading'].values\n",
    "    \n",
    "    # Find segment boundaries\n",
    "    changes = np.diff(trading, prepend=0)\n",
    "    starts = np.where(changes == 1)[0]  # Trading starts\n",
    "    stops = np.where(changes == -1)[0]  # Trading stops\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if len(starts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # If data starts with trading\n",
    "    if trading[0] == 1:\n",
    "        starts = np.insert(starts, 0, 0)\n",
    "    \n",
    "    # If data ends with trading\n",
    "    if trading[-1] == 1:\n",
    "        stops = np.append(stops, len(trading))\n",
    "    \n",
    "    # Ensure we have matching starts and stops\n",
    "    min_len = min(len(starts), len(stops))\n",
    "    starts = starts[:min_len]\n",
    "    stops = stops[:min_len]\n",
    "    \n",
    "    # Extract segments\n",
    "    segments = []\n",
    "    for start, stop in zip(starts, stops):\n",
    "        segment_length = stop - start\n",
    "        if segment_length >= min_segment_length:\n",
    "            segment = df.iloc[start:stop].copy()\n",
    "            segments.append(segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "def analyze_asset_segments(train_dir, min_segment_length=96):\n",
    "    \"\"\"\n",
    "    Analyze all assets and extract their trading segments.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: {\n",
    "        'asset_name': {\n",
    "            'segments': [list of dataframes],\n",
    "            'total_rows': int,\n",
    "            'trading_rows': int,\n",
    "            'n_segments': int,\n",
    "            'avg_segment_length': float\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    asset_files = [f for f in os.listdir(train_dir) if f.endswith('.parquet')]\n",
    "    asset_segments = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRADING SEGMENT EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Minimum segment length: {min_segment_length} rows\")\n",
    "    print(f\"(For 15min data: {min_segment_length * 15 / 60:.1f} hours)\")\n",
    "    print()\n",
    "    \n",
    "    for asset_file in asset_files:\n",
    "        asset_name = asset_file.replace('.parquet', '')\n",
    "        df = pd.read_parquet(os.path.join(train_dir, asset_file))\n",
    "        \n",
    "        # Extract segments\n",
    "        segments = extract_trading_segments(df, min_segment_length)\n",
    "        \n",
    "        if len(segments) > 0:\n",
    "            total_rows = len(df)\n",
    "            trading_rows = sum(len(seg) for seg in segments)\n",
    "            avg_length = trading_rows / len(segments)\n",
    "            \n",
    "            asset_segments[asset_name] = {\n",
    "                'segments': segments,\n",
    "                'total_rows': total_rows,\n",
    "                'trading_rows': trading_rows,\n",
    "                'n_segments': len(segments),\n",
    "                'avg_segment_length': avg_length,\n",
    "                'trading_ratio_original': (df['is_trading'] == 1).mean(),\n",
    "                'trading_ratio_extracted': 1.0  # 100% after extraction\n",
    "            }\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    return asset_segments\n",
    "\n",
    "\n",
    "def select_best_segments(asset_segments, target_total_rows=50000, target_n_assets=5):\n",
    "    \"\"\"\n",
    "    Select best segments to reach target total rows with maximum diversity.\n",
    "    \n",
    "    Strategy:\n",
    "    - Prioritize longer segments (more continuous data)\n",
    "    - Ensure diversity across different assets\n",
    "    - Reach target total rows\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SELECTING SEGMENTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Target total rows: {target_total_rows:,}\")\n",
    "    print(f\"Target n_assets: {target_n_assets}\")\n",
    "    print()\n",
    "    \n",
    "    # Sort assets by total usable trading rows\n",
    "    sorted_assets = sorted(\n",
    "        asset_segments.items(),\n",
    "        key=lambda x: x[1]['trading_rows'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    selected = {}\n",
    "    selected_asset_names = []\n",
    "    total_rows_selected = 0\n",
    "    \n",
    "    # First pass: Select top assets until we hit target rows\n",
    "    for asset_name, info in sorted_assets:\n",
    "        if len(selected) >= target_n_assets:\n",
    "            break\n",
    "        \n",
    "        asset_trading_rows = info['trading_rows']\n",
    "        \n",
    "        # Add this asset\n",
    "        selected[asset_name] = info\n",
    "        selected_asset_names.append(asset_name)\n",
    "        total_rows_selected += asset_trading_rows\n",
    "        \n",
    "        print(f\"✓ Selected {asset_name:<20} \"\n",
    "              f\"Segments: {info['n_segments']:>3} | \"\n",
    "              f\"Rows: {asset_trading_rows:>8,} | \"\n",
    "              f\"Cumulative: {total_rows_selected:>10,}\")\n",
    "        \n",
    "        if total_rows_selected >= target_total_rows:\n",
    "            print(f\"\\n✓ Reached target of {target_total_rows:,} rows!\")\n",
    "            break\n",
    "    \n",
    "    if total_rows_selected < target_total_rows:\n",
    "        print(f\"\\n⚠️  Only found {total_rows_selected:,} rows (target: {target_total_rows:,})\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SELECTION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Assets selected: {len(selected)}\")\n",
    "    print(f\"Total segments: {sum(info['n_segments'] for info in selected.values())}\")\n",
    "    print(f\"Total rows: {total_rows_selected:,}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return selected, selected_asset_names\n",
    "\n",
    "\n",
    "def build_timeseries_from_segments(selected_segments, val_dir, target_cols, \n",
    "                                   input_chunk_length, output_chunk_length,selected_asset_names):\n",
    "    \"\"\"\n",
    "    Build Darts TimeSeries from selected trading segments.\n",
    "    \"\"\"\n",
    "    train_ts_list = []\n",
    "    val_ts_list = []\n",
    "    past_covariates_train_list = []\n",
    "    past_covariates_val_list = []\n",
    "    val_dfs_list = []\n",
    "    total_windows = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BUILDING TIMESERIES FROM SEGMENTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for asset_name, info in selected_segments.items():\n",
    "        segments = info['segments']\n",
    "        \n",
    "        print(f\"\\nProcessing {asset_name}: {len(segments)} segments\")\n",
    "        \n",
    "        # Concatenate all segments for this asset\n",
    "        # Note: This creates artificial discontinuities, but all data is trading data\n",
    "        train_df_combined = pd.concat(segments, ignore_index=True)\n",
    "        \n",
    "        # Load validation data\n",
    "        val_path = os.path.join(val_dir, f\"{asset_name}.parquet\")\n",
    "        if os.path.exists(val_path):\n",
    "            val_df = pd.read_parquet(val_path)\n",
    "            \n",
    "            # Extract only trading segments from validation too\n",
    "            val_segments = extract_trading_segments(val_df, min_segment_length=96)\n",
    "            if len(val_segments) > 0:\n",
    "                val_df_combined = pd.concat(val_segments, ignore_index=True)\n",
    "            else:\n",
    "                print(f\"    No trading segments in validation, using full data\")\n",
    "                val_df_combined = val_df\n",
    "        else:\n",
    "            print(f\"    No validation file, using train data\")\n",
    "            val_df_combined = train_df_combined\n",
    "        \n",
    "        # Clean timestamps\n",
    "        train_df_combined['ExecutionTime'] = pd.to_datetime(\n",
    "            train_df_combined['ExecutionTime']\n",
    "        ).dt.tz_localize(None)\n",
    "        val_df_combined['ExecutionTime'] = pd.to_datetime(\n",
    "            val_df_combined['ExecutionTime']\n",
    "        ).dt.tz_localize(None)\n",
    "        \n",
    "        # Clean numeric data & Convert to float32\n",
    "        numeric_cols = train_df_combined.select_dtypes(include=[np.number]).columns\n",
    "        for df in [train_df_combined, val_df_combined]:\n",
    "            df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "            df[numeric_cols] = df[numeric_cols].ffill().bfill().fillna(0)\n",
    "            df[numeric_cols] = df[numeric_cols].astype(np.float32)\n",
    "\n",
    "        val_dfs_list.append(val_df_combined)\n",
    "        \n",
    "        # Get covariate columns\n",
    "        covariate_cols = [\n",
    "            col for col in train_df_combined.columns \n",
    "            if col not in target_cols + ['ExecutionTime', 'is_trading']\n",
    "        ]\n",
    "        \n",
    "        # Create TimeSeries\n",
    "        # Note: We need to handle the discontinuous time index\n",
    "        # Option 1: Reset time index to be continuous (losing real timestamps)\n",
    "        # Option 2: Keep real timestamps but accept gaps\n",
    "        \n",
    "        # Using Option 1 for stability\n",
    "        train_df_continuous = train_df_combined.copy()\n",
    "        val_df_continuous = val_df_combined.copy()\n",
    "        \n",
    "        # Create continuous 15-min frequency index\n",
    "        train_df_continuous['ExecutionTime'] = pd.date_range(\n",
    "            start='2020-01-01', \n",
    "            periods=len(train_df_continuous), \n",
    "            freq='15min'\n",
    "        )\n",
    "        val_df_continuous['ExecutionTime'] = pd.date_range(\n",
    "            start='2020-01-01', \n",
    "            periods=len(val_df_continuous), \n",
    "            freq='15min'\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Create TimeSeries\n",
    "            train_ts = TimeSeries.from_dataframe(\n",
    "                train_df_continuous, \n",
    "                time_col='ExecutionTime', \n",
    "                value_cols=target_cols, \n",
    "                freq='15min'\n",
    "            )\n",
    "            val_ts = TimeSeries.from_dataframe(\n",
    "                val_df_continuous, \n",
    "                time_col='ExecutionTime', \n",
    "                value_cols=target_cols, \n",
    "                freq='15min'\n",
    "            )\n",
    "            \n",
    "            train_ts_list.append(train_ts)\n",
    "            val_ts_list.append(val_ts)\n",
    "            \n",
    "            # Add covariates if available\n",
    "            if len(covariate_cols) > 0:\n",
    "                past_cov_train = TimeSeries.from_dataframe(\n",
    "                    train_df_continuous, \n",
    "                    time_col='ExecutionTime', \n",
    "                    value_cols=covariate_cols, \n",
    "                    freq='15min'\n",
    "                )\n",
    "                past_cov_val = TimeSeries.from_dataframe(\n",
    "                    val_df_continuous, \n",
    "                    time_col='ExecutionTime', \n",
    "                    value_cols=covariate_cols, \n",
    "                    freq='15min'\n",
    "                )\n",
    "                past_covariates_train_list.append(past_cov_train)\n",
    "                past_covariates_val_list.append(past_cov_val)\n",
    "            else:\n",
    "                past_covariates_train_list.append(None)\n",
    "                past_covariates_val_list.append(None)\n",
    "            \n",
    "            # Calculate windows\n",
    "            n_windows = max(0, len(train_df_combined) - input_chunk_length - output_chunk_length + 1)\n",
    "            total_windows += n_windows\n",
    "            \n",
    "            print(f\"  ✓ Created TimeSeries: {len(train_df_combined):,} rows → {n_windows:,} windows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to create TimeSeries: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TIMESERIES BUILD COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total assets: {len(train_ts_list)}\")\n",
    "    print(f\"Total windows: {total_windows:,}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'train_ts_list': train_ts_list,\n",
    "        'val_ts_list': val_ts_list,\n",
    "        'past_covariates_train_list': past_covariates_train_list,\n",
    "        'past_covariates_val_list': past_covariates_val_list,\n",
    "        'val_dfs_list': val_dfs_list,\n",
    "        'total_windows': total_windows,\n",
    "        'selected_assets': selected_asset_names,\n",
    "        'avg_variance': 0.0\n",
    "    }\n",
    "\n",
    "def build_timeseries_data(\n",
    "    input_chunk_length, output_chunk_length, \n",
    "    target_cols, train_dir, val_dir,\n",
    "    target_total_rows=50000,\n",
    "    target_n_assets=5,\n",
    "    min_segment_length=96\n",
    "):\n",
    "    \"\"\"\n",
    "    Build training data using ONLY trading periods (aggressive extraction).\n",
    "    \n",
    "    This approach:\n",
    "    1. Extracts continuous trading segments from each asset\n",
    "    2. Filters out all non-trading periods\n",
    "    3. Ensures 100% of training data is actual trading activity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_chunk_length : int\n",
    "        Input sequence length\n",
    "    output_chunk_length : int\n",
    "        Output sequence length\n",
    "    target_cols : list\n",
    "        Target column names\n",
    "    train_dir : str\n",
    "        Training data directory\n",
    "    val_dir : str\n",
    "        Validation data directory\n",
    "    target_total_rows : int\n",
    "        Target number of total rows (default: 50,000)\n",
    "    target_n_assets : int\n",
    "        Target number of assets to use (default: 5)\n",
    "    min_segment_length : int\n",
    "        Minimum trading segment length in rows (default: 96 = 24 hours for 15min)\n",
    "    \"\"\"\n",
    "    global PREBUILT_DATA, TUNING_ASSETS\n",
    "    \n",
    "    if PREBUILT_DATA is not None:\n",
    "        print(\"Reusing pre-built data...\")\n",
    "        return PREBUILT_DATA\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRADING-ONLY DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Analyze all assets and extract segments\n",
    "    asset_segments = analyze_asset_segments(train_dir, min_segment_length)\n",
    "\n",
    "    if len(asset_segments) == 0:\n",
    "        raise ValueError(\"No assets with sufficient trading segments found!\")\n",
    "    \n",
    "    # Step 2: Select best segments\n",
    "    selected_segments, selected_asset_names = select_best_segments(\n",
    "        asset_segments, \n",
    "        target_total_rows=target_total_rows,\n",
    "        target_n_assets=target_n_assets\n",
    "    )\n",
    "\n",
    "    # Store selected assets globally\n",
    "    TUNING_ASSETS = selected_asset_names\n",
    "    \n",
    "    # Step 3: Build TimeSeries from segments\n",
    "    PREBUILT_DATA = build_timeseries_from_segments(\n",
    "        selected_segments,\n",
    "        val_dir,\n",
    "        target_cols,\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        selected_asset_names\n",
    "    )\n",
    "    \n",
    "    # Verification\n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"✓ Assets: {len(PREBUILT_DATA['train_ts_list'])}\")\n",
    "    print(f\"✓ Total windows: {PREBUILT_DATA['total_windows']:,}\")\n",
    "    print(f\"✓ Training data is 100% trading periods\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return PREBUILT_DATA\n",
    "\n",
    "# To create filtered parquet files\n",
    "def create_trading_only_files(train_dir, val_dir, output_train_dir, output_val_dir, assets_list):\n",
    "    \"\"\"Create new parquet files with only trading segments.\"\"\"\n",
    "    os.makedirs(output_train_dir, exist_ok=True)\n",
    "    os.makedirs(output_val_dir, exist_ok=True)\n",
    "    \n",
    "    for asset_name in assets_list:\n",
    "        # Process training file\n",
    "        train_file = os.path.join(train_dir, f\"{asset_name}.parquet\")\n",
    "        if os.path.exists(train_file):\n",
    "            df = pd.read_parquet(train_file)\n",
    "            segments = extract_trading_segments(df, min_segment_length=96)\n",
    "            \n",
    "            if segments:\n",
    "                df_trading = pd.concat(segments, ignore_index=True)\n",
    "                output_path = os.path.join(output_train_dir, f\"{asset_name}.parquet\")\n",
    "                df_trading.to_parquet(output_path)\n",
    "                print(f\"✓ Created training file for {asset_name}\")\n",
    "\n",
    "        # Process validation file\n",
    "        val_file = os.path.join(val_dir, f\"{asset_name}.parquet\")\n",
    "        if os.path.exists(val_file):\n",
    "            df = pd.read_parquet(val_file)\n",
    "            segments = extract_trading_segments(df, min_segment_length=96)\n",
    "            \n",
    "            if segments:\n",
    "                df_trading = pd.concat(segments, ignore_index=True)\n",
    "                output_path = os.path.join(output_val_dir, f\"{asset_name}.parquet\")\n",
    "                df_trading.to_parquet(output_path)\n",
    "                print(f\"✓ Created validation file for {asset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563e7ff-24e0-4316-847e-74e008e7b487",
   "metadata": {},
   "source": [
    "## 3. Chronos Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7553f4c7-ee09-4bfa-a362-49e444c9336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers used by objective_chronos2 ---\n",
    "def _to_tensor(x):\n",
    "    \"\"\"Convert x (tensor / numpy / list/tuple of tensors or scalars) -> torch.Tensor (cpu).\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.cpu()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.from_numpy(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        if len(x) == 0:\n",
    "            raise ValueError(\"Empty list cannot be converted to tensor.\")\n",
    "        if all(isinstance(el, torch.Tensor) for el in x):\n",
    "            return torch.stack([el.cpu() for el in x])\n",
    "        if all(isinstance(el, np.ndarray) for el in x):\n",
    "            return torch.from_numpy(np.stack(x))\n",
    "        # fallback: attempt elementwise conversion then stack\n",
    "        converted = []\n",
    "        for el in x:\n",
    "            if isinstance(el, torch.Tensor):\n",
    "                converted.append(el.cpu())\n",
    "            elif isinstance(el, np.ndarray):\n",
    "                converted.append(torch.from_numpy(el))\n",
    "            else:\n",
    "                converted.append(torch.tensor(el))\n",
    "        return torch.stack(converted)\n",
    "    # final fallback\n",
    "    return torch.tensor(x)\n",
    "\n",
    "def _extract_context_target_mask(batch):\n",
    "    \"\"\"Return (context, target, future_mask_or_None) from common batch formats.\"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        ctx_keys = (\"past_target\", \"past_targets\", \"past\", \"history\", \"x\")\n",
    "        tgt_keys = (\"future_target\", \"future_targets\", \"future\", \"y\", \"target\")\n",
    "        mask_keys = (\"future_mask\", \"mask\", \"future_masks\")\n",
    "        context = next((batch[k] for k in ctx_keys if k in batch), None)\n",
    "        target = next((batch[k] for k in tgt_keys if k in batch), None)\n",
    "        future_mask = next((batch[k] for k in mask_keys if k in batch), None)\n",
    "        return context, target, future_mask\n",
    "\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        if len(batch) == 2:\n",
    "            return batch[0], batch[1], None\n",
    "        if len(batch) >= 3:\n",
    "            context = batch[0]\n",
    "            target = batch[2] if len(batch) > 2 else None\n",
    "            future_mask = batch[3] if len(batch) > 3 else None\n",
    "            return context, target, future_mask\n",
    "\n",
    "    raise TypeError(f\"Unsupported batch format: {type(batch)}\")\n",
    "\n",
    "def _to_torch(x):\n",
    "    \"\"\"Convert pipeline outputs (list/ndarray/tensor) -> torch.Tensor on CPU or None if x is None.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.cpu()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.from_numpy(x)\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 0:\n",
    "            return None\n",
    "        if all(isinstance(el, np.ndarray) for el in x):\n",
    "            return torch.from_numpy(np.stack(x))\n",
    "        if all(isinstance(el, torch.Tensor) for el in x):\n",
    "            return torch.stack([el.cpu() for el in x])\n",
    "        # fallback to tensor construct\n",
    "        return torch.tensor(x)\n",
    "    raise TypeError(f\"Unsupported type for forecast output conversion: {type(x)}\")\n",
    "\n",
    "def align_forecast_to_target(q_out: torch.Tensor | None,\n",
    "                             mean_out: torch.Tensor | None,\n",
    "                             n_features: int,\n",
    "                             pred_len: int,\n",
    "                             quantile_levels: list) -> torch.Tensor | None:\n",
    "    \"\"\"\n",
    "    Align forecast to [B, pred_len, n_features]. Returns CPU tensor or None.\n",
    "    Handles common layouts including [B, n_var, pred_len, n_q] and [B, pred_len, n_q, n_var].\n",
    "    \"\"\"\n",
    "    median_idx = len(quantile_levels) // 2\n",
    "\n",
    "    if q_out is not None:\n",
    "        if q_out.ndim == 4:\n",
    "            B, a, b, c = q_out.shape\n",
    "            # case: [B, n_var, pred_len, n_q]\n",
    "            if a == n_features and b == pred_len and c == len(quantile_levels):\n",
    "                med = q_out[..., median_idx]           # [B, n_var, pred_len]\n",
    "                return med.permute(0, 2, 1).contiguous()  # -> [B, pred_len, n_var]\n",
    "            # case: [B, pred_len, n_q, n_var]\n",
    "            if a == pred_len and b == len(quantile_levels) and c == n_features:\n",
    "                return q_out[:, :, median_idx, :].contiguous()  # [B, pred_len, n_var]\n",
    "            # try reasonable permutations\n",
    "            for perm in [(0,1,2,3),(0,2,1,3),(0,3,1,2),(0,2,3,1)]:\n",
    "                try:\n",
    "                    cand = q_out.permute(*perm)\n",
    "                    if cand.ndim == 4 and cand.shape[1] == pred_len and cand.shape[3] == n_features and cand.shape[2] == len(quantile_levels):\n",
    "                        return cand[:, :, median_idx, :].contiguous()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        elif q_out.ndim == 3:\n",
    "            s = q_out.shape\n",
    "            # [B, pred_len, n_var]\n",
    "            if s[1] == pred_len and s[2] == n_features:\n",
    "                return q_out.contiguous()\n",
    "            # [B, n_var, pred_len]\n",
    "            if s[1] == n_features and s[2] == pred_len:\n",
    "                return q_out.permute(0, 2, 1).contiguous()\n",
    "            # [B, n_var, n_q] -> pick median quantile and expand (best-effort)\n",
    "            if s[1] == n_features and s[2] == len(quantile_levels):\n",
    "                med = q_out[:, :, median_idx]  # [B, n_var]\n",
    "                return med.unsqueeze(1).expand(-1, pred_len, -1).contiguous()\n",
    "\n",
    "    # fallback to mean_out\n",
    "    if mean_out is not None and isinstance(mean_out, torch.Tensor):\n",
    "        if mean_out.ndim == 3:\n",
    "            if mean_out.shape[1] == n_features and mean_out.shape[2] == pred_len:\n",
    "                return mean_out.permute(0, 2, 1).contiguous()\n",
    "            if mean_out.shape[1] == pred_len and mean_out.shape[2] == n_features:\n",
    "                return mean_out.contiguous()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832ec22e-1c3e-448f-bab9-878826e3c5a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRADING-ONLY DATA EXTRACTION\n",
      "================================================================================\n",
      "================================================================================\n",
      "TRADING SEGMENT EXTRACTION\n",
      "================================================================================\n",
      "Minimum segment length: 96 rows\n",
      "(For 15min data: 24.0 hours)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SELECTING SEGMENTS\n",
      "================================================================================\n",
      "Target total rows: 200,000\n",
      "Target n_assets: 5\n",
      "\n",
      "✓ Selected Thu23Q3              Segments: 105 | Rows:   81,980 | Cumulative:     81,980\n",
      "✓ Selected Thu23Q2              Segments: 104 | Rows:   81,908 | Cumulative:    163,888\n",
      "✓ Selected Thu23Q1              Segments: 104 | Rows:   81,849 | Cumulative:    245,737\n",
      "\n",
      "✓ Reached target of 200,000 rows!\n",
      "\n",
      "================================================================================\n",
      "SELECTION SUMMARY\n",
      "================================================================================\n",
      "Assets selected: 3\n",
      "Total segments: 313\n",
      "Total rows: 245,737\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BUILDING TIMESERIES FROM SEGMENTS\n",
      "================================================================================\n",
      "\n",
      "Processing Thu23Q3: 105 segments\n",
      "  ✓ Created TimeSeries: 81,980 rows → 81,923 windows\n",
      "\n",
      "Processing Thu23Q2: 104 segments\n",
      "  ✓ Created TimeSeries: 81,908 rows → 81,851 windows\n",
      "\n",
      "Processing Thu23Q1: 104 segments\n",
      "  ✓ Created TimeSeries: 81,849 rows → 81,792 windows\n",
      "\n",
      "================================================================================\n",
      "TIMESERIES BUILD COMPLETE\n",
      "================================================================================\n",
      "Total assets: 3\n",
      "Total windows: 245,566\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL VERIFICATION\n",
      "================================================================================\n",
      "✓ Assets: 3\n",
      "✓ Total windows: 245,566\n",
      "✓ Training data is 100% trading periods\n",
      "================================================================================\n",
      "\n",
      "✓ Created training file for Thu23Q3\n",
      "✓ Created validation file for Thu23Q3\n",
      "✓ Created training file for Thu23Q2\n",
      "✓ Created validation file for Thu23Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-31 01:55:46,126] A new study created in memory with name: Chronos2_multivariate_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created training file for Thu23Q1\n",
      "✓ Created validation file for Thu23Q1\n",
      "\n",
      "✓ Created filtered trading-only files for 3 assets\n",
      "\n",
      "============================================================\n",
      "Starting Chronos-2 Multivariate Optimization\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44921d9087e43c5848449a51ecfc55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 0\n",
      "============================================================\n",
      "Trial 0: val_loss=0.507279, time=535.6s\n",
      "[I 2025-10-31 02:04:43,332] Trial 0 finished with value: 0.5072793900966645 and parameters: {'quantile_levels': 'large', 'batch_size': 32, 'lr': 1e-05, 'max_context_length': 96}. Best is trial 0 with value: 0.5072793900966645.\n",
      "\n",
      "============================================================\n",
      "Trial 1\n",
      "============================================================\n",
      "Trial 1: val_loss=0.489217, time=536.2s\n",
      "[I 2025-10-31 02:13:41,514] Trial 1 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 2\n",
      "============================================================\n",
      "Trial 2: val_loss=0.518395, time=534.2s\n",
      "[I 2025-10-31 02:22:37,773] Trial 2 finished with value: 0.5183945670723915 and parameters: {'quantile_levels': 'medium', 'batch_size': 16, 'lr': 3e-05, 'max_context_length': 384}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 3\n",
      "============================================================\n",
      "Trial 3: val_loss=0.489217, time=545.3s\n",
      "[I 2025-10-31 02:31:45,139] Trial 3 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 4\n",
      "============================================================\n",
      "Trial 4: val_loss=0.489217, time=549.4s\n",
      "[I 2025-10-31 02:40:56,613] Trial 4 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'small', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 5\n",
      "============================================================\n",
      "Trial 5: val_loss=0.489217, time=547.6s\n",
      "[I 2025-10-31 02:50:06,269] Trial 5 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0003, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 6\n",
      "============================================================\n",
      "Trial 6: val_loss=0.507279, time=547.5s\n",
      "[I 2025-10-31 02:59:15,936] Trial 6 finished with value: 0.5072793900966645 and parameters: {'quantile_levels': 'large', 'batch_size': 32, 'lr': 0.0001, 'max_context_length': 384}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 7\n",
      "============================================================\n",
      "Trial 7: val_loss=0.518395, time=551.0s\n",
      "[I 2025-10-31 03:08:28,967] Trial 7 finished with value: 0.5183945670723915 and parameters: {'quantile_levels': 'small', 'batch_size': 16, 'lr': 3e-05, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 8\n",
      "============================================================\n",
      "Trial 8: val_loss=0.489217, time=550.6s\n",
      "[I 2025-10-31 03:17:41,616] Trial 8 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 1e-05, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 9\n",
      "============================================================\n",
      "Trial 9: val_loss=0.489217, time=552.3s\n",
      "[I 2025-10-31 03:26:55,949] Trial 9 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0003, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 10\n",
      "============================================================\n",
      "Trial 10: val_loss=0.507279, time=550.2s\n",
      "[I 2025-10-31 03:36:08,240] Trial 10 finished with value: 0.5072793900966645 and parameters: {'quantile_levels': 'small', 'batch_size': 32, 'lr': 0.0001, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 11\n",
      "============================================================\n",
      "Trial 11: val_loss=0.489217, time=552.9s\n",
      "[I 2025-10-31 03:45:23,221] Trial 11 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 12\n",
      "============================================================\n",
      "Trial 12: val_loss=0.489217, time=552.8s\n",
      "[I 2025-10-31 03:54:38,126] Trial 12 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 13\n",
      "============================================================\n",
      "Trial 13: val_loss=0.489217, time=553.4s\n",
      "[I 2025-10-31 04:03:53,559] Trial 13 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 14\n",
      "============================================================\n",
      "Trial 14: val_loss=0.489217, time=551.3s\n",
      "[I 2025-10-31 04:13:06,893] Trial 14 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0001, 'max_context_length': 384}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 15\n",
      "============================================================\n",
      "Trial 15: val_loss=0.518395, time=545.8s\n",
      "[I 2025-10-31 04:22:14,804] Trial 15 finished with value: 0.5183945670723915 and parameters: {'quantile_levels': 'large', 'batch_size': 16, 'lr': 0.0001, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 16\n",
      "============================================================\n",
      "Trial 16: val_loss=0.489217, time=550.1s\n",
      "[I 2025-10-31 04:31:26,995] Trial 16 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 0.0003, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 17\n",
      "============================================================\n",
      "Trial 17: val_loss=0.489217, time=551.9s\n",
      "[I 2025-10-31 04:40:40,920] Trial 17 finished with value: 0.4892168939113617 and parameters: {'quantile_levels': 'medium', 'batch_size': 64, 'lr': 1e-05, 'max_context_length': 192}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 18\n",
      "============================================================\n",
      "Trial 18: val_loss=0.518395, time=549.1s\n",
      "[I 2025-10-31 04:49:52,058] Trial 18 finished with value: 0.5183945670723915 and parameters: {'quantile_levels': 'small', 'batch_size': 16, 'lr': 3e-05, 'max_context_length': 96}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Trial 19\n",
      "============================================================\n",
      "Trial 19: val_loss=0.507279, time=550.0s\n",
      "[I 2025-10-31 04:59:04,078] Trial 19 finished with value: 0.5072793900966645 and parameters: {'quantile_levels': 'large', 'batch_size': 32, 'lr': 0.0001, 'max_context_length': 384}. Best is trial 1 with value: 0.4892168939113617.\n",
      "\n",
      "============================================================\n",
      "Chronos-2 Multivariate Optimization Complete!\n",
      "============================================================\n",
      "\n",
      "Best trial: 1\n",
      "Best validation loss: 0.489217\n",
      "\n",
      "--- Best Chronos-2 Hyperparameters ---\n",
      "  quantile_levels: medium\n",
      "  batch_size: 64\n",
      "  lr: 0.0001\n",
      "  max_context_length: 96\n",
      "  Actual quantile levels used: [0.05, 0.25, 0.5, 0.75, 0.95]\n",
      "\n",
      "Timing Statistics:\n",
      "  Total optimization time: 183.3 minutes\n",
      "  Total training time: 183.1 minutes\n",
      "  Average trial duration: 549.4s\n",
      "\n",
      "Trial Statistics:\n",
      "  Total trials: 20\n",
      "  Completed: 20\n",
      "  Pruned: 0\n",
      "  Failed: 0\n",
      "\n",
      "Results saved to ..\\results\n",
      "\n",
      "Top 5 Trials:\n",
      "  1. Trial 1: loss=0.489217, time=537.6s, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n",
      "  2. Trial 3: loss=0.489217, time=546.8s, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n",
      "  3. Trial 4: loss=0.489217, time=551.0s, quantiles=[0.1, 0.5, 0.9]\n",
      "  4. Trial 5: loss=0.489217, time=549.1s, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n",
      "  5. Trial 8: loss=0.489217, time=552.1s, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n"
     ]
    }
   ],
   "source": [
    "if PREBUILT_DATA is None:\n",
    "    data = build_timeseries_data(\n",
    "        input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "        output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "        target_cols=TARGET_COLS,\n",
    "        train_dir=TRAIN_DIR,\n",
    "        val_dir=VAL_DIR,\n",
    "        target_total_rows=200000,\n",
    "        target_n_assets=5,\n",
    "        min_segment_length=96\n",
    "    )\n",
    "else:\n",
    "    data = PREBUILT_DATA\n",
    "    print(f\"Reusing pre-built data: {data['total_windows']} windows, {len(data['train_ts_list'])} assets\")\n",
    "\n",
    "# Create trading-only files for Chronos (which uses the datamodule)\n",
    "if TUNING_ASSETS is not None:\n",
    "    create_trading_only_files(\n",
    "        TRAIN_DIR, VAL_DIR, \n",
    "        TRAIN_DIR_FILTERED, VAL_DIR_FILTERED,\n",
    "        TUNING_ASSETS\n",
    "    )\n",
    "    print(f\"\\n✓ Created filtered trading-only files for {len(TUNING_ASSETS)} assets\")\n",
    "\n",
    "def objective_chronos2(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Chronos-2 hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Hyperparameters to tune\n",
    "    quantile_levels_choice = trial.suggest_categorical(\"quantile_levels\", [\"small\", \"medium\", \"large\"])\n",
    "    quantile_levels_map = {\n",
    "        \"small\": [0.1, 0.5, 0.9],\n",
    "        \"medium\": [0.05, 0.25, 0.5, 0.75, 0.95],\n",
    "        \"large\": [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "    }\n",
    "    quantile_levels = quantile_levels_map[quantile_levels_choice]\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-5, 3e-5, 1e-4, 3e-4])\n",
    "\n",
    "    # Additional hyperparameters specific to your use case\n",
    "    max_context_length = trial.suggest_categorical(\"max_context_length\", [96, 192, 384])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\nTrial {trial.number}\\n{'='*60}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # choose device for loss computation\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Load pretrained pipeline (keeps model on cuda if possible per from_pretrained device_map)\n",
    "        pipeline = Chronos2Pipeline.from_pretrained(\n",
    "            \"amazon/chronos-2\",\n",
    "            device_map=\"cuda\" if torch.cuda.is_available() else None,\n",
    "            dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "\n",
    "        # Create datamodule with hyperparameters\n",
    "        datamodule = ElectricityDataModule(\n",
    "            train_parquet=TRAIN_DIR_FILTERED,\n",
    "            val_parquet=VAL_DIR_FILTERED,\n",
    "            scalers_dir=SCALERS_DIR,\n",
    "            batch_size = batch_size,\n",
    "            dataset_kwargs={\n",
    "                'input_chunk_length': min(INPUT_CHUNK_LENGTH, max_context_length),\n",
    "                'output_chunk_length': OUTPUT_CHUNK_LENGTH,\n",
    "                'target_cols': TARGET_COLS,\n",
    "                'assets_list': TUNING_ASSETS\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Setup datamodule\n",
    "        datamodule.setup()\n",
    "        \n",
    "        # Get validation loader\n",
    "        val_loader = datamodule.val_dataloader()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "        max_batches = 20  # Limit for speed\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                if batch_idx >= max_batches:\n",
    "                    break\n",
    "                    \n",
    "                # extract raw\n",
    "                try:\n",
    "                    raw_context, raw_target, raw_mask = _extract_context_target_mask(batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting context/target from batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # convert to tensors (CPU)\n",
    "                try:\n",
    "                    context = _to_tensor(raw_context)          # [B, context_len, n_features]\n",
    "                    target = _to_tensor(raw_target) if raw_target is not None else None\n",
    "                    future_mask = _to_tensor(raw_mask) if raw_mask is not None else None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting batch elements to tensors (batch {batch_idx}): {e}\")\n",
    "                    continue\n",
    "\n",
    "                # ensure dims\n",
    "                if context.ndim == 2:\n",
    "                    context = context.unsqueeze(-1)\n",
    "                if target is not None and target.ndim == 2:\n",
    "                    target = target.unsqueeze(-1)\n",
    "\n",
    "\n",
    "                # shapes and params\n",
    "                try:\n",
    "                    B = context.shape[0]\n",
    "                    context_length = context.shape[1]\n",
    "                    n_features = context.shape[2]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading shapes from context (batch {batch_idx}): {e}\")\n",
    "                    continue\n",
    "\n",
    "                # prepare input for pipeline: [B, n_features, history]\n",
    "                multivariate_context = context.transpose(1, 2)   # CPU tensor\n",
    "\n",
    "                # call pipeline (requires numpy input on CPU)\n",
    "                try:\n",
    "                    q_out_raw, mean_out_raw = pipeline.predict_quantiles(\n",
    "                        multivariate_context.cpu().numpy(),\n",
    "                        prediction_length=OUTPUT_CHUNK_LENGTH,\n",
    "                        quantile_levels=quantile_levels\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"pipeline.predict_quantiles error (batch {batch_idx}): {e}\")\n",
    "                    continue\n",
    "\n",
    "                # convert pipeline outputs to CPU torch tensors\n",
    "                try:\n",
    "                    q_out = _to_torch(q_out_raw)    # CPU\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not convert quantiles output to tensor: {e}\")\n",
    "                    continue\n",
    "\n",
    "                mean_out = None\n",
    "                if mean_out_raw is not None:\n",
    "                    try:\n",
    "                        mean_out = _to_torch(mean_out_raw)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not convert mean output to tensor: {e}\")\n",
    "                        mean_out = None\n",
    "\n",
    "                # align forecast to [B, pred_len, n_features]\n",
    "                forecast_median = align_forecast_to_target(q_out, mean_out, n_features, OUTPUT_CHUNK_LENGTH, quantile_levels)\n",
    "                if forecast_median is None:\n",
    "                    print(f\"Shape mismatch after alignment (batch {batch_idx}): could not align forecast {getattr(q_out,'shape',None)} vs target {getattr(target,'shape',None)}\")\n",
    "                    continue\n",
    "\n",
    "                # final sanity: shapes must match\n",
    "                if target is None:\n",
    "                    print(f\"Missing target for batch {batch_idx}; skipping\")\n",
    "                    continue\n",
    "                if forecast_median.shape != target.shape:\n",
    "                    print(f\"Shape mismatch after alignment (batch {batch_idx}): forecast {forecast_median.shape} vs target {target.shape}\")\n",
    "                    continue\n",
    "\n",
    "                # Move tensors to computation device (GPU preferred) for loss computation\n",
    "                forecast_median = forecast_median.to(device)\n",
    "                target = target.to(device)\n",
    "                future_mask = (future_mask.to(device) if future_mask is not None else torch.ones_like(target).to(device))\n",
    "\n",
    "                # compute loss\n",
    "                try:\n",
    "                    loss = masked_smoothed_smape(forecast_median, target, future_mask)\n",
    "                    batch_loss = float(loss.item())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing loss on batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                total_loss += batch_loss\n",
    "                total_batches += 1   \n",
    "\n",
    "        avg_loss = total_loss / total_batches if total_batches > 0 else float('inf')\n",
    "        fit_time = time.time() - start_time\n",
    "        print(f\"Trial {trial.number}: val_loss={avg_loss:.6f}, time={fit_time:.1f}s\")\n",
    "                    \n",
    "        trial.set_user_attr(\"actual_quantile_levels\", quantile_levels)\n",
    "        return avg_loss                \n",
    "\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"Trial {trial.number}: CUDA OOM\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return float('inf')\n",
    "        else:\n",
    "            print(f\"Trial {trial.number}: Runtime error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number}: Failed with {type(e).__name__}: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "    finally:\n",
    "        if 'pipeline' in locals():\n",
    "            try:\n",
    "                del pipeline\n",
    "            except Exception:\n",
    "                pass\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting Chronos-2 Multivariate Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Optuna study\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=3, seed=SEED)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=2)\n",
    "\n",
    "study_chronos2 = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"Chronos2_multivariate_tuning\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "try:\n",
    "    study_chronos2.optimize(\n",
    "        objective_chronos2,\n",
    "        n_trials=N_TRIALS_Chronos,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization interrupted by user\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Chronos-2 Multivariate Optimization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process results\n",
    "completed_trials = [t for t in study_chronos2.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pruned_trials = [t for t in study_chronos2.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "failed_trials = [t for t in study_chronos2.trials if t.state == optuna.trial.TrialState.FAIL]\n",
    "\n",
    "if len(completed_trials) > 0:\n",
    "    best_params_chronos2 = study_chronos2.best_params\n",
    "    best_value = study_chronos2.best_value\n",
    "    \n",
    "    # Get the actual quantile levels used in the best trial\n",
    "    best_trial = study_chronos2.best_trial\n",
    "    actual_quantile_levels = best_trial.user_attrs.get(\"actual_quantile_levels\", None)\n",
    "    \n",
    "    print(f\"\\nBest trial: {best_trial.number}\")\n",
    "    print(f\"Best validation loss: {best_value:.6f}\")\n",
    "    print(\"\\n--- Best Chronos-2 Hyperparameters ---\")\n",
    "    for key, value in best_params_chronos2.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    if actual_quantile_levels:\n",
    "        print(f\"  Actual quantile levels used: {actual_quantile_levels}\")\n",
    "    \n",
    "    trial_durations = [t.duration.total_seconds() for t in completed_trials if t.duration]\n",
    "    if trial_durations:\n",
    "        avg_duration = np.mean(trial_durations)\n",
    "        total_training_time = sum(trial_durations)\n",
    "        print(f\"\\nTiming Statistics:\")\n",
    "        print(f\"  Total optimization time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"  Total training time: {total_training_time/60:.1f} minutes\")\n",
    "        print(f\"  Average trial duration: {avg_duration:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nTrial Statistics:\")\n",
    "    print(f\"  Total trials: {len(study_chronos2.trials)}\")\n",
    "    print(f\"  Completed: {len(completed_trials)}\")\n",
    "    print(f\"  Pruned: {len(pruned_trials)}\")\n",
    "    print(f\"  Failed: {len(failed_trials)}\")\n",
    "    \n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save results with actual quantile levels\n",
    "    save_params = best_params_chronos2.copy()\n",
    "    if actual_quantile_levels:\n",
    "        save_params['actual_quantile_levels'] = actual_quantile_levels\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"best_params_chronos2.json\"), \"w\") as f:\n",
    "        json.dump(save_params, f, indent=2)\n",
    "    \n",
    "    stats = {\n",
    "        'best_value': best_value,\n",
    "        'best_params': save_params,\n",
    "        'best_trial': best_trial.number,\n",
    "        'n_trials': len(study_chronos2.trials),\n",
    "        'n_completed': len(completed_trials),\n",
    "        'n_pruned': len(pruned_trials),\n",
    "        'n_failed': len(failed_trials),\n",
    "        'total_time_seconds': total_time\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"chronos2_study_stats.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to {RESULTS_DIR}\")\n",
    "    \n",
    "    if len(completed_trials) > 4:\n",
    "        print(\"\\nTop 5 Trials:\")\n",
    "        sorted_trials = sorted(completed_trials, key=lambda t: t.value)[:5]\n",
    "        for i, trial in enumerate(sorted_trials, 1):\n",
    "            duration = trial.duration.total_seconds() if trial.duration else 0\n",
    "            actual_ql = trial.user_attrs.get(\"actual_quantile_levels\", \"N/A\")\n",
    "            print(f\"  {i}. Trial {trial.number}: loss={trial.value:.6f}, time={duration:.1f}s, quantiles={actual_ql}\")\n",
    "else:\n",
    "    print(\"\\nNo trials completed successfully!\")\n",
    "    print(f\"Failed trials: {len(failed_trials)}\")\n",
    "    print(f\"Pruned trials: {len(pruned_trials)}\")\n",
    "    best_params_chronos2 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5284a12-7272-4c56-8988-3437979b7171",
   "metadata": {},
   "source": [
    "There actually many trials with the same loss. All of those trials have the batch size of 64 and all but one use medium quantile levels.The learing rates and max context length varies. To speed the training and hopefully the convergence, I'll pick the largest learning rates and mininun max context length among those successfull trials. This will require a manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de567c26-02ca-4747-bf3a-9fc92844b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_chronos2 = {\n",
    "  \"quantile_levels\": \"medium\",\n",
    "  \"batch_size\": 64,\n",
    "  \"lr\": 0.0003,\n",
    "  \"max_context_length\": 96,\n",
    "  \"actual_quantile_levels\": [\n",
    "    0.05,\n",
    "    0.25,\n",
    "    0.5,\n",
    "    0.75,\n",
    "    0.95\n",
    "  ]\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"best_params_chronos2.json\"), \"w\") as f:\n",
    "    json.dump(save_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ee4a3-2217-4533-8a7f-367088392a9a",
   "metadata": {},
   "source": [
    "## 4. LSTM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd09b12-fd8a-4684-af59-4df803c4a694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merta\\OneDrive\\Desktop\\FS Courses\\Deep Learning\\Final_Project\\.venv\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "[I 2025-10-31 04:59:04,748] A new study created in memory with name: LSTM_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing pre-built data: 245566 windows, 3 assets\n",
      "\n",
      "============================================================\n",
      "Starting LSTM Optimization\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45007783dbc64f4388b5367267aab206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 0\n",
      "============================================================\n",
      "Params: hidden=128, layers=4, \n",
      "        dropout=0.100, lr=0.01000, batch=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: val_loss=22.747440, \n",
      "              epochs=6/10, time=1550.0s\n",
      "[I 2025-10-31 05:24:54,758] Trial 0 finished with value: 22.747440338134766 and parameters: {'hidden_dim': 128, 'n_rnn_layers': 4, 'dropout': 0.1, 'lr': 0.01, 'batch_size': 32}. Best is trial 0 with value: 22.747440338134766.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 1\n",
      "============================================================\n",
      "Params: hidden=32, layers=2, \n",
      "        dropout=0.400, lr=0.00030, batch=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: val_loss=11.313666, \n",
      "              epochs=11/10, time=2493.1s\n",
      "[I 2025-10-31 06:06:28,393] Trial 1 finished with value: 11.313666343688965 and parameters: {'hidden_dim': 32, 'n_rnn_layers': 2, 'dropout': 0.4, 'lr': 0.0003, 'batch_size': 32}. Best is trial 1 with value: 11.313666343688965.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 2\n",
      "============================================================\n",
      "Params: hidden=128, layers=3, \n",
      "        dropout=0.100, lr=0.00300, batch=128\n",
      "Trial 2: val_loss=9.943198, \n",
      "              epochs=6/10, time=445.0s\n",
      "[I 2025-10-31 06:13:53,957] Trial 2 finished with value: 9.943198204040527 and parameters: {'hidden_dim': 128, 'n_rnn_layers': 3, 'dropout': 0.1, 'lr': 0.003, 'batch_size': 128}. Best is trial 2 with value: 9.943198204040527.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 3\n",
      "============================================================\n",
      "Params: hidden=128, layers=4, \n",
      "        dropout=0.000, lr=0.00300, batch=128\n",
      "Trial 3: val_loss=11.340981, \n",
      "              epochs=10/10, time=860.8s\n",
      "[I 2025-10-31 06:28:15,317] Trial 3 finished with value: 11.340981483459473 and parameters: {'hidden_dim': 128, 'n_rnn_layers': 4, 'dropout': 0.0, 'lr': 0.003, 'batch_size': 128}. Best is trial 2 with value: 9.943198204040527.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Trial 4\n",
      "============================================================\n",
      "Params: hidden=256, layers=3, \n",
      "        dropout=0.000, lr=0.00300, batch=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4: val_loss=10.689938, \n",
      "              epochs=11/10, time=1935.4s\n",
      "[I 2025-10-31 07:00:31,287] Trial 4 finished with value: 10.689937591552734 and parameters: {'hidden_dim': 256, 'n_rnn_layers': 3, 'dropout': 0.0, 'lr': 0.003, 'batch_size': 64}. Best is trial 2 with value: 9.943198204040527.\n",
      "\n",
      "============================================================\n",
      "Trial 5\n",
      "============================================================\n",
      "Params: hidden=64, layers=2, \n",
      "        dropout=0.100, lr=0.00100, batch=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5: val_loss=8.530094, \n",
      "              epochs=9/10, time=153.9s\n",
      "[I 2025-10-31 07:03:05,367] Trial 5 finished with value: 8.530094146728516 and parameters: {'hidden_dim': 64, 'n_rnn_layers': 2, 'dropout': 0.1, 'lr': 0.001, 'batch_size': 128}. Best is trial 5 with value: 8.530094146728516.\n",
      "\n",
      "============================================================\n",
      "Trial 6\n",
      "============================================================\n",
      "Params: hidden=64, layers=2, \n",
      "        dropout=0.200, lr=0.00100, batch=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6: val_loss=9.693222, \n",
      "              epochs=9/10, time=81.4s\n",
      "[I 2025-10-31 07:04:26,869] Trial 6 finished with value: 9.693222045898438 and parameters: {'hidden_dim': 64, 'n_rnn_layers': 2, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 128}. Best is trial 5 with value: 8.530094146728516.\n",
      "\n",
      "============================================================\n",
      "Trial 7\n",
      "============================================================\n",
      "Params: hidden=64, layers=2, \n",
      "        dropout=0.300, lr=0.00010, batch=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7: Pruned\n",
      "[I 2025-10-31 07:05:39,035] Trial 7 pruned. Trial pruned at epoch 3\n",
      "\n",
      "============================================================\n",
      "Trial 8\n",
      "============================================================\n",
      "Params: hidden=64, layers=2, \n",
      "        dropout=0.100, lr=0.00100, batch=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected user-defined float16-like precision. For mixed precision training, recommended options are 'bf16-mixed' and '16-mixed'.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8: val_loss=8.530094, \n",
      "              epochs=9/10, time=83.4s\n",
      "[I 2025-10-31 07:07:02,533] Trial 8 finished with value: 8.530094146728516 and parameters: {'hidden_dim': 64, 'n_rnn_layers': 2, 'dropout': 0.1, 'lr': 0.001, 'batch_size': 128}. Best is trial 5 with value: 8.530094146728516.\n",
      "\n",
      "============================================================\n",
      "Trial 9\n",
      "============================================================\n",
      "Params: hidden=32, layers=3, \n",
      "        dropout=0.400, lr=0.00100, batch=128\n",
      "Trial 9: Pruned\n",
      "[I 2025-10-31 07:07:50,370] Trial 9 pruned. Trial pruned at epoch 3\n",
      "\n",
      "============================================================\n",
      "Optimization Complete!\n",
      "============================================================\n",
      "\n",
      "Best trial: 5\n",
      "Best validation loss: 8.530094\n",
      "\n",
      "--- Best LSTM Hyperparameters ---\n",
      "  hidden_dim: 64\n",
      "  n_rnn_layers: 2\n",
      "  dropout: 0.1\n",
      "  lr: 0.001\n",
      "  batch_size: 128\n",
      "\n",
      "Timing Statistics:\n",
      "  Total optimization time: 128.8 minutes\n",
      "  Total training time: 126.7 minutes\n",
      "  Average trial duration: 950.4s\n",
      "\n",
      "Trial Statistics:\n",
      "  Total trials: 10\n",
      "  Completed: 8\n",
      "  Pruned: 2\n",
      "  Failed: 0\n",
      "\n",
      "Results saved to ..\\results\n",
      "\n",
      "Top 5 Trials:\n",
      "  1. Trial 5: loss=8.530094, time=153.9s\n",
      "  2. Trial 8: loss=8.530094, time=83.4s\n",
      "  3. Trial 6: loss=9.693222, time=81.4s\n",
      "  4. Trial 2: loss=9.943198, time=445.0s\n",
      "  5. Trial 4: loss=10.689938, time=1935.4s\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "if PREBUILT_DATA is None:\n",
    "    data = build_timeseries_data(\n",
    "        input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "        output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "        target_cols=TARGET_COLS,\n",
    "        train_dir=TRAIN_DIR,\n",
    "        val_dir=VAL_DIR,\n",
    "        target_total_rows=200000,\n",
    "        target_n_assets=5,\n",
    "        min_segment_length=96\n",
    "    )\n",
    "else:\n",
    "    data = PREBUILT_DATA\n",
    "    print(f\"Reusing pre-built data: {data['total_windows']} windows, {len(data['train_ts_list'])} assets\")\n",
    "\n",
    "train_ts_list = data['train_ts_list']\n",
    "val_ts_list = data['val_ts_list']\n",
    "past_covariates_train_list = data['past_covariates_train_list']\n",
    "past_covariates_val_list = data['past_covariates_val_list']\n",
    "\n",
    "class PyTorchLightningPruningCallback(pl.Callback):\n",
    "    \"\"\"Prune trials that perform poorly early.\"\"\"\n",
    "    \n",
    "    def __init__(self, trial, monitor=\"val_loss\"):\n",
    "        super().__init__()\n",
    "        self.trial = trial\n",
    "        self.monitor = monitor\n",
    "    \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        current_score = trainer.callback_metrics.get(self.monitor)\n",
    "        if current_score is None:\n",
    "            return\n",
    "        \n",
    "        self.trial.report(float(current_score), epoch)\n",
    "        \n",
    "        if self.trial.should_prune():\n",
    "            message = f\"Trial pruned at epoch {epoch}\"\n",
    "            raise optuna.TrialPruned(message)\n",
    "\n",
    "def objective_lstm_optimized(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optimized LSTM objective using BlockRNNModel for multi-step forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 2, 4) \n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.0, 0.1, 0.2, 0.3, 0.4])\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-4, 3e-4, 1e-3, 3e-3, 1e-2])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Trial {trial.number}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Params: hidden={hidden_dim}, layers={n_rnn_layers}, \")\n",
    "    print(f\"        dropout={dropout:.3f}, lr={lr:.5f}, batch={batch_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        trial_id = f\"trial_{trial.number:03d}\"\n",
    "        ckpt_dir = f\"./checkpoints/LSTM_optuna/{trial_id}\"\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            mode='min',\n",
    "            min_delta=1e-4,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "        \n",
    "        max_epochs = 10\n",
    "\n",
    "        trainer_kwargs = {\n",
    "            'accelerator': 'gpu',\n",
    "            'devices': 1,\n",
    "            'precision': '16-mixed',\n",
    "            'max_epochs': max_epochs,\n",
    "            'gradient_clip_val': 1.0,\n",
    "            'gradient_clip_algorithm': 'norm',\n",
    "            'enable_progress_bar': False,\n",
    "            'enable_model_summary': False,\n",
    "            'logger': False,\n",
    "            'callbacks': [early_stopping, pruning_callback],\n",
    "            'benchmark': True,\n",
    "            'deterministic': False,\n",
    "            'val_check_interval': 1.0,\n",
    "            'num_sanity_val_steps': 0\n",
    "        }\n",
    "        \n",
    "        model = BlockRNNModel(\n",
    "            model=\"LSTM\",\n",
    "            input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "            output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_rnn_layers=n_rnn_layers,\n",
    "            dropout=dropout,\n",
    "            batch_size=batch_size,\n",
    "            n_epochs=max_epochs,\n",
    "            loss_fn=torch.nn.L1Loss(),\n",
    "            optimizer_kwargs={'lr': lr},\n",
    "            model_name=f\"LSTM_trial_{trial.number}\",\n",
    "            save_checkpoints=True,\n",
    "            work_dir=ckpt_dir,\n",
    "            force_reset=True,\n",
    "            random_state=SEED,\n",
    "            pl_trainer_kwargs=trainer_kwargs\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            series=train_ts_list,\n",
    "            past_covariates=past_covariates_train_list,\n",
    "            val_series=val_ts_list,\n",
    "            val_past_covariates=past_covariates_val_list,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - start_time\n",
    "        \n",
    "        if hasattr(model, 'trainer') and model.trainer is not None:\n",
    "            val_loss = model.trainer.callback_metrics.get(\"val_loss\", None)\n",
    "            \n",
    "            if val_loss is None:\n",
    "                print(f\"Trial {trial.number}: No val_loss found\")\n",
    "                return float('inf')\n",
    "            \n",
    "            val_loss_value = float(val_loss.item() if torch.is_tensor(val_loss) else val_loss)\n",
    "            \n",
    "            if not np.isfinite(val_loss_value):\n",
    "                print(f\"Trial {trial.number}: Invalid val_loss={val_loss_value}\")\n",
    "                return float('inf')\n",
    "            \n",
    "            actual_epochs = model.trainer.current_epoch + 1\n",
    "            \n",
    "            print(f\"Trial {trial.number}: val_loss={val_loss_value:.6f}, \")\n",
    "            print(f\"              epochs={actual_epochs}/{max_epochs}, time={fit_time:.1f}s\")\n",
    "            \n",
    "            return val_loss_value\n",
    "        else:\n",
    "            print(f\"Trial {trial.number}: Trainer not accessible\")\n",
    "            return float('inf')\n",
    "            \n",
    "    except optuna.TrialPruned:\n",
    "        print(f\"Trial {trial.number}: Pruned\")\n",
    "        raise\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"Trial {trial.number}: CUDA OOM\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return float('inf')\n",
    "        else:\n",
    "            print(f\"Trial {trial.number}: Runtime error: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number}: Failed with {type(e).__name__}: {e}\")\n",
    "        return float('inf')\n",
    "    \n",
    "    finally:\n",
    "        if 'model' in locals():\n",
    "            try:\n",
    "                del model\n",
    "            except:\n",
    "                pass\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting LSTM Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=5, seed=SEED)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=3, interval_steps=1)\n",
    "\n",
    "study_lstm = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"LSTM_tuning\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "try:\n",
    "    study_lstm.optimize(\n",
    "        objective_lstm_optimized,\n",
    "        n_trials=N_TRIALS_LSTM,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization interrupted by user\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Optimization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completed_trials = [t for t in study_lstm.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pruned_trials = [t for t in study_lstm.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "failed_trials = [t for t in study_lstm.trials if t.state == optuna.trial.TrialState.FAIL]\n",
    "\n",
    "if len(completed_trials) > 0:\n",
    "    best_params_lstm = study_lstm.best_params\n",
    "    best_value = study_lstm.best_value\n",
    "    \n",
    "    print(f\"\\nBest trial: {study_lstm.best_trial.number}\")\n",
    "    print(f\"Best validation loss: {best_value:.6f}\")\n",
    "    print(\"\\n--- Best LSTM Hyperparameters ---\")\n",
    "    for key, value in best_params_lstm.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    trial_durations = [t.duration.total_seconds() for t in completed_trials if t.duration]\n",
    "    if trial_durations:\n",
    "        avg_duration = np.mean(trial_durations)\n",
    "        total_training_time = sum(trial_durations)\n",
    "        print(f\"\\nTiming Statistics:\")\n",
    "        print(f\"  Total optimization time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"  Total training time: {total_training_time/60:.1f} minutes\")\n",
    "        print(f\"  Average trial duration: {avg_duration:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nTrial Statistics:\")\n",
    "    print(f\"  Total trials: {len(study_lstm.trials)}\")\n",
    "    print(f\"  Completed: {len(completed_trials)}\")\n",
    "    print(f\"  Pruned: {len(pruned_trials)}\")\n",
    "    print(f\"  Failed: {len(failed_trials)}\")\n",
    "    \n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"best_params_lstm.json\"), \"w\") as f:\n",
    "        json.dump(best_params_lstm, f, indent=2)\n",
    "    \n",
    "    stats = {\n",
    "        'best_value': best_value,\n",
    "        'best_params': best_params_lstm,\n",
    "        'best_trial': study_lstm.best_trial.number,\n",
    "        'n_trials': len(study_lstm.trials),\n",
    "        'n_completed': len(completed_trials),\n",
    "        'n_pruned': len(pruned_trials),\n",
    "        'n_failed': len(failed_trials),\n",
    "        'total_time_seconds': total_time\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"lstm_study_stats.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to {RESULTS_DIR}\")\n",
    "    \n",
    "    if len(completed_trials) > 4:\n",
    "        print(\"\\nTop 5 Trials:\")\n",
    "        sorted_trials = sorted(completed_trials, key=lambda t: t.value)[:5]\n",
    "        for i, trial in enumerate(sorted_trials, 1):\n",
    "            duration = trial.duration.total_seconds() if trial.duration else 0\n",
    "            print(f\"  {i}. Trial {trial.number}: loss={trial.value:.6f}, time={duration:.1f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo trials completed successfully!\")\n",
    "    print(f\"Failed trials: {len(failed_trials)}\")\n",
    "    print(f\"Pruned trials: {len(pruned_trials)}\")\n",
    "    best_params_lstm = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379687a-f36a-46c8-99e4-2ded9dd4d179",
   "metadata": {},
   "source": [
    "## 5. Save Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c0639d-9254-4ab2-a5de-2ed7096821f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters saved to: ..\\results\\best_hyperparameters_tst.json\n",
      "--- Contents ---\n",
      "{\n",
      "    \"chronos\": {\n",
      "        \"quantile_levels\": \"medium\",\n",
      "        \"batch_size\": 64,\n",
      "        \"lr\": 0.0003,\n",
      "        \"max_context_length\": 96,\n",
      "        \"actual_quantile_levels\": [\n",
      "            0.05,\n",
      "            0.25,\n",
      "            0.5,\n",
      "            0.75,\n",
      "            0.95\n",
      "        ]\n",
      "    },\n",
      "    \"LSTM\": {\n",
      "        \"hidden_dim\": 64,\n",
      "        \"n_rnn_layers\": 2,\n",
      "        \"dropout\": 0.1,\n",
      "        \"lr\": 0.001,\n",
      "        \"batch_size\": 128\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"chronos\": best_params_chronos2,\n",
    "    \"LSTM\": best_params_lstm,\n",
    "}\n",
    "output_path = os.path.join(RESULTS_DIR, \"best_hyperparameters.json\")\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "print(f\"Best hyperparameters saved to: {output_path}\")\n",
    "print('--- Contents ---')\n",
    "print(json.dumps(best_params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271413cf-dea4-487a-ac16-9e447d2580e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
